# AI Tool Signatures

Patterns and fingerprints left by AI code generation tools. Used by the `ai-patterns` analyzer to detect AI-generated code.

## Why Detect AI-Generated Code?

AI-generated code isn't inherently bad. The issue is that AI tools produce code that *looks correct* but often lacks:

1. **Consistent error handling** — Silent catches, generic error messages
2. **Defensive coding** — Missing edge cases, optimistic assumptions
3. **Architectural coherence** — Each file works in isolation but doesn't fit the larger system
4. **Testing** — AI rarely generates meaningful tests alongside features
5. **Security awareness** — Functional code that doesn't consider adversarial inputs

Detecting AI-generated patterns helps teams focus review effort where it matters most.

## Direct Fingerprints

These are explicit attributions left by tools or their users.

| Pattern | Tool | Where Found |
|---------|------|-------------|
| `Generated by Cursor` | Cursor | Comments, usually at file top |
| `Created with Bolt` | Bolt.new | Comments, README |
| `Built with Lovable` | Lovable (formerly GPT Engineer) | Comments, README, meta tags |
| `Generated by Copilot` | GitHub Copilot | Comments |
| `Co-authored-by:.*copilot` | GitHub Copilot | Git commit messages |
| `Generated by ChatGPT` | ChatGPT | Comments |
| `Generated by Claude` | Claude / Claude Code | Comments |
| `v0.dev` | Vercel v0 | Comments, component files |
| `Generated by Codeium` | Codeium | Comments |
| `Generated by Tabnine` | Tabnine | Comments |
| `windsurf` | Windsurf (Codeium) | Comments, config |

## Behavioral Signatures

These patterns are statistically associated with AI-generated code. They're not proof of AI generation, but they correlate strongly — especially in combination.

### Silent Catch Blocks

```javascript
// AI tools generate this constantly
try {
  await fetchData();
} catch (error) {
  // empty — error silently swallowed
}
```

AI code generators prioritize "no errors in the console" over "errors are handled correctly." This pattern is the #1 indicator of unreviewed AI code.

### Generic Error Messages

```javascript
throw new Error('Something went wrong');
throw new Error('An error occurred');
throw new Error('Failed');
```

When AI tools do add error handling, they use vague, unhelpful messages. Human developers who wrote the code know what the error is and describe it.

### Excessive Try-Catch Wrapping

```javascript
try {
  const data = JSON.parse(input);
  try {
    await processData(data);
    try {
      await saveResult(result);
    } catch (e) { console.error(e); }
  } catch (e) { console.error(e); }
} catch (e) { console.error(e); }
```

AI tools wrap every operation in its own try-catch, creating deeply nested error handling that obscures the actual logic.

### Commented-Out Code

```javascript
// const oldApproach = doThingOldWay();
// if (condition) {
//   handleOldCase();
// }
const newApproach = doThingNewWay();
```

AI tools iterate by commenting out previous attempts instead of deleting them. Human developers using version control delete old code.

### Inconsistent Naming

```javascript
const user_data = fetchUserProfile();  // snake_case
const isActive = userData.active;       // camelCase
const UserStatus = getStatus();         // PascalCase
```

When code is assembled from multiple AI prompts, each generation may use different conventions. A human-written codebase tends to be internally consistent.

### Near-Duplicate Files

AI tools often generate similar-but-not-identical files for similar prompts. You'll see:
- Multiple API route handlers with identical structure but different entity names
- Form components that share 80% of their code
- Utility files that duplicate logic available in existing project modules

### TypeScript `any` Overuse

```typescript
function processData(input: any): any {
  const result: any = transform(input);
  return result;
}
```

AI tools use `any` to avoid complex type inference. This defeats the purpose of TypeScript.

### TODO/FIXME Accumulation

```javascript
// TODO: implement error handling
// FIXME: this should validate input
// TODO: add proper types
```

A high density of unresolved TODO comments often indicates AI-generated code that was accepted without completing the implementation.

## Detection Confidence

| Signal | Alone | Combined (2+) |
|--------|-------|---------------|
| Direct fingerprint | High | High |
| Silent catches (3+ per file) | Medium | High |
| Generic errors + commented code | Low | Medium |
| Inconsistent naming | Low | Medium |
| Near-duplicate files | Medium | High |
| any overuse + no tests | Low | Medium |

**Important:** These signals inform review prioritization. They should not be used to blame or shame. AI tools are legitimate development aids — the issue is when generated code ships without review.

## What to Do About It

1. **Don't remove AI tools.** They're productivity multipliers when used well.
2. **Require review.** AI-generated code needs the same review standards as human code.
3. **Fix the patterns.** Silent catches, generic errors, and missing tests are real issues regardless of origin.
4. **Establish conventions.** Documented style guides help AI tools generate more consistent code.
5. **Test first.** Write tests before asking AI to implement, or require AI to generate tests alongside implementation.
